{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gift Giving\n",
    "### Finding the perfect gift\n",
    "---\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "What should you buy for mother's day? Your mom never really asks for anything and it seems like she has everything she wants, though you know that can't be true. Plus, you feel pressure to get her something she will really like but it seems so difficult that you want to give up.\n",
    "\n",
    "It is hard to come up with a meaningful gift. If you are anything like me, you love to think of the perfect suprise and watch for the look of suprise and happiness.\n",
    "\n",
    "**Gifts matter**\n",
    "\n",
    "An effective gift can strengthen a relationship and build friendship between the giver and receiver. \n",
    "\n",
    "\n",
    "**The Problem with Presents**\n",
    "\n",
    "In, \"Money Can't Buy Love\" we learn that the amount of money spent on a gift is not correlated with the satisfaction it generates. Wanting to impress the receiver, gift givers often overspend. Unfortunately, this often produces a gift the receiver doesn't really want. Though anthropologists label gift giving a positive social process, economists label it as an objective waste of resources. (i.e. The deadweight loss of Christmas)\n",
    "\n",
    "Several psycology papers highlight this unfortunate disconnect between the giver and the receiver. Some hypothesize that people tend to have trouble learning from their own gift receiving experiences.\n",
    "\n",
    "It takes time to think of a gift and often, we may not know what the person wants at all. We may end up sacrificing a stronger relationship for the time we save when we give a lame gift.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "Using the tools of data science and new psycology studies, we can provide assistance to gift givers and help them spend less time finding a meaningful gift.\n",
    "\n",
    "### Quantify Meaningful Gifts\n",
    "---\n",
    "Recent studies highlight three key principles that can lead to a meaningful gift.\n",
    "\n",
    "1. Give something that reflects their personal interests\n",
    "2. Give something they can actually use\n",
    "3. Give something that provides long lasting value\n",
    "\n",
    "(see \"Why Certain Gifts Are Great to Give but Not to Get\")\n",
    "\n",
    "Next we need data to analyze people's interests, the usefullness of the gifts and how long they last. We will start by determining people's interests.\n",
    "\n",
    "Amazon public wishlist data is availible online. Because it is owned by Amazon, we can't use it commercially, but we can use it in research and development. We will begin by writing a script that scrapes Amazon wishlists. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Wishlist Webscraper\n",
    "\n",
    "The following code searches through Amazon wishlists and retrives item names, prices and categories. Unfortunately, Amazon does not allow their data to be distributed so this code is only for personal use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I collected the top ten names for each decade from the US social security web page. We will read them in and use them to search the Amazon lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sarah', 'Charles', 'Heather', 'Thomas', 'Richard', 'Robert', 'Patricia', 'Abigail', 'Megan', 'Brian', 'Matthew', 'Melissa', 'Joseph', 'Ethan', 'Linda', 'Ashley', 'Nicholas', 'Mary', 'Deborah', 'Amy', 'Jeffrey', 'John', 'Elizabeth', 'Donna', 'Hannah', 'Samantha', 'Andrew', 'William', 'Olivia', 'Jacob', 'Christopher', 'Michelle', 'Isabella', 'Emma', 'Susan', 'James', 'Michael', 'Brittany', 'David', 'Stephanie', 'Angela', 'Nancy', 'Lisa', 'Barbara', 'Mark', 'Amanda', 'Jessica', 'Joshua', 'Madison', 'Taylor', 'Jason', 'Kimberly', 'Debra', 'Cynthia', 'Jennifer', 'Tyler', 'Karen', 'Daniel', 'Nicole', 'Emily']\n"
     ]
    }
   ],
   "source": [
    "topNames = set()\n",
    "\n",
    "#Open the top names file and parse\n",
    "file = open('topNames.txt')\n",
    "lines = [l.split() for l in file.readlines()]\n",
    "\n",
    "#Handpicked indices for each line\n",
    "#based on file\n",
    "for l in lines:\n",
    "    topNames.add(l[1])\n",
    "    topNames.add(l[3])\n",
    "\n",
    "topNames = list(topNames)\n",
    "print(topNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the swapping out the names in the wishlist lookup URL we can find pages of random people and their wishlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/gp/registry/search/ref=cm_wl_search__sortbar_page_2?ie=UTF8&field-firstname=&field-lastname=&field-name=Robert&index=us-xml-wishlist&page=2&submit.search=1\n"
     ]
    }
   ],
   "source": [
    "#The url for searching wish lists\n",
    "wishlistSearch = 'https://www.amazon.com/gp/registry/search/'\n",
    "siteVar = 'ref=cm_wl_search__sortbar_page_2?ie=UTF8&field-firstname=&field-lastname=&'\n",
    "changeVar = 'field-name={}&index=us-xml-wishlist&page={}&submit.search=1'\n",
    "\n",
    "#Put together the pieces:\n",
    "searchPage = wishlistSearch+siteVar+changeVar.format(topNames[5],2)\n",
    "print(searchPage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above url works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for scraping each portion\n",
    "\n",
    "** 1. Find a bunch of random people **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSearchResults(searchPage,browser):\n",
    "    #Open the search page\n",
    "    browser.get(searchPage)\n",
    "    searchResultsHTML = browser.page_source\n",
    "    soup = BeautifulSoup(searchResultsHTML,'html.parser')\n",
    "    \n",
    "    #Create an empty dictionary\n",
    "    namesUrls = {'Name':[],'Place':[],'userUrls':[]}\n",
    "    \n",
    "    #Find the first wishlist box\n",
    "    name_box = soup.find('div', attrs={'class':\n",
    "\"a-box a-spacing-top-medium a-color-base-background a-text-left people-box\"})\n",
    "    \n",
    "    while name_box:\n",
    "        #Get the name and user url\n",
    "        textSection = name_box.find('div',attrs={'class':\n",
    "                 'a-section a-spacing-none a-spacing-top-none'})\n",
    "        namesUrls['userUrls'].append(textSection.find('a').get('href'))\n",
    "        namesUrls['Name'].append(textSection.find('a').text.strip())\n",
    "\n",
    "\n",
    "        #If displayed, get where the user is from\n",
    "        item1 = None\n",
    "        item2 = None\n",
    "        \n",
    "        textSection = textSection.find_next_sibling()\n",
    "        if textSection:\n",
    "            item1 = textSection.text\n",
    "            textSection = textSection.find_next_sibling()\n",
    "            if textSection:\n",
    "                item2 = textSection.text\n",
    "\n",
    "        #Check which item is a birthday and which is a place\n",
    "        if item1 and item2:\n",
    "            place = item2.strip()\n",
    "        else:\n",
    "            try: \n",
    "                int(item1.strip()[-1])\n",
    "                place = None\n",
    "            except ValueError:\n",
    "                place = item1.strip()\n",
    "            except TypeError:\n",
    "                place = None\n",
    "            except AttributeError:\n",
    "                place = None\n",
    "                \n",
    "        namesUrls['Place'].append(place)\n",
    "        \n",
    "        #Iterate\n",
    "        name_box = name_box.find_next_sibling()\n",
    "    \n",
    "    #Rest\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return namesUrls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Get people's wishlists **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWishlistUrls(userUrl,browser):\n",
    "    \"\"\"Parses the user list page and returns\n",
    "    the wish lists urls\n",
    "    \"\"\"\n",
    "    #To store wishlist urls\n",
    "    wishlists = []\n",
    "    \n",
    "    try:\n",
    "        browser.get(userUrl)\n",
    "        userPageHTML = browser.page_source\n",
    "        soup = BeautifulSoup(userPageHTML,'html.parser')\n",
    "        listsBox = soup.find('div', attrs={'id':'my-lists-tab'})\n",
    "        if listsBox:\n",
    "            listsBox = listsBox.find('div',attrs={\"aria-expanded\":\"true\"})\n",
    "            for l in listsBox.findAll('a'): \n",
    "                wishlists.append(str(l.get('href')))\n",
    "        \n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    \n",
    "    #Rest\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return wishlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.Get all the items in each wishlist **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseWishlist(wishlistUrl,browser):\n",
    "    allItems = {\n",
    "        'Name':[],\n",
    "        'Price':[],\n",
    "        'Url':[]\n",
    "    }\n",
    "    try:\n",
    "        browser.get(wishlistUrl)\n",
    "        time.sleep(3)\n",
    "        #Scroll to the bottom of page with arbitarily large number\n",
    "        browser.execute_script(\"window.scrollTo(0,10000000)\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    \n",
    "    #Seach through the HTML for the data\n",
    "    listPageHTML = browser.page_source\n",
    "    soup = BeautifulSoup(listPageHTML,'html.parser')\n",
    "\n",
    "    gifts = soup.find('ul', attrs={'id':'g-items'})\n",
    "    try:\n",
    "        item = gifts.find('li')\n",
    "\n",
    "        while item:\n",
    "            #Find the name and url\n",
    "            nameLink = item.find('a', attrs={'class':'a-link-normal'})\n",
    "\n",
    "            #Store the data\n",
    "            if nameLink:\n",
    "                allItems['Name'].append(nameLink.get('title'))\n",
    "                allItems['Url'].append(nameLink.get('href'))\n",
    "                allItems['Price'].append(item.get('data-price'))\n",
    "\n",
    "            #iterate\n",
    "            item = item.find_next('li',attrs={'class':'a-spacing-none g-item-sortable'})\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "    \n",
    "    return allItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Get the item category for each item**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategory(itemUrl,browser):\n",
    "    #Find the category\n",
    "    try:\n",
    "        browser.get(itemUrl)\n",
    "        soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "    except TimeoutException:\n",
    "        return ''\n",
    "    \n",
    "    breadcrumb = soup.find('div',attrs={'id':'wayfinding-breadcrumbs_container'})\n",
    "    \n",
    "    if breadcrumb:\n",
    "        #Process Strings\n",
    "        category = breadcrumb.text\n",
    "        category = category.split('\\n')\n",
    "        category = [c.strip() for c in category]\n",
    "        category = ', '.join(category)\n",
    "        return category\n",
    "    \n",
    "    else:\n",
    "        return ''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Browser Object\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "#Urls\n",
    "amazon = 'https://www.amazon.com'\n",
    "wishlistSearch = amazon + '/gp/registry/search/ref=cm_wl_search__\\\n",
    "sortbar_page_2?ie=UTF8&field-firstname=&field-lastname=&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the data in an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sql database to store data\n",
    "\n",
    "db = sqlite3.connect(\"amazonData.SQL\")\n",
    "cur = db.cursor()\n",
    "\n",
    "#Create tables to store data\n",
    "#cur.execute(\"\"\"DROP TABLE IF EXISTS people \"\"\")\n",
    "#cur.execute(\"\"\"DROP TABLE IF EXISTS wishlists \"\"\")\n",
    "#cur.execute(\"\"\"DROP TABLE IF EXISTS items \"\"\")\n",
    "\n",
    "#cur.execute(\"\"\"CREATE TABLE people (ID TEXT, Name TEXT,Place TEXT,userUrls TEXT)\"\"\")\n",
    "#cur.execute(\"\"\"CREATE TABLE wishlists (ID TEXT, wishlistUrl TEXT)\"\"\")\n",
    "#cur.execute(\"\"\"CREATE TABLE items (ID TEXT, Name TEXT, Price FLOAT, Url TEXT, Category TEXT)\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute Function 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Browser Object\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "#Urls\n",
    "amazon = 'https://www.amazon.com'\n",
    "wishlistSearch = amazon + '/gp/registry/search/ref=cm_wl_search__\\\n",
    "sortbar_page_2?ie=UTF8&field-firstname=&field-lastname=&'\n",
    "\n",
    "#Static Variable\n",
    "nextID = 1\n",
    "\n",
    "#Urls with variables to edit\n",
    "changeVar = 'field-name={}&index=us-xml-wishlist&page={}&submit.search=1'\n",
    "\n",
    "#Loop through five random pages of search results for each name\n",
    "for commonName in topNames:\n",
    "    for pageNum in np.random.randint(40,size=5):\n",
    "        searchPage = wishlistSearch+changeVar.format(commonName,pageNum)\n",
    "        peopleInfo = parseSearchResults(searchPage,browser)\n",
    "        \n",
    "        #Create ID numbers\n",
    "        numUsers = len(peopleInfo['Name'])\n",
    "        ID = [str(id) for id in range(nextID,nextID+numUsers)]\n",
    "        nextID += numUsers + 1\n",
    "\n",
    "        #Store User Data\n",
    "        peopleInfo['userUrls'] = [amazon+url for url in peopleInfo['userUrls']]\n",
    "        peopleTuple = zip(ID,peopleInfo['Name'],\n",
    "                            peopleInfo['Place'],\n",
    "                            peopleInfo['userUrls'])\n",
    "        cur.executemany(\"INSERT INTO people VALUES(?,?,?,?)\",peopleTuple)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute Function 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each user and find all wishlists\n",
    "cur.execute(\"SELECT ID, userUrls FROM people\")\n",
    "\n",
    "for  idUrl in cur.fetchall():\n",
    "    #Scrape Data\n",
    "    ID,userUrl = idUrl\n",
    "    wishlists = findWishlistUrls(userUrl,browser)\n",
    "    \n",
    "    #Store Data\n",
    "    wishlists = [amazon+wl for wl in wishlists]\n",
    "    wlTuple = zip([ID]*len(wishlists),wishlists)\n",
    "    cur.executemany(\"INSERT INTO wishlists VALUES(?,?)\",wlTuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute Function 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each wishlist to find all items\n",
    "cur.execute(\"SELECT ID, wishlistUrl FROM wishlists\")\n",
    "\n",
    "for idUrl in cur.fetchall():\n",
    "    #Scrape data\n",
    "    ID,wlUrl = idUrl\n",
    "    items = parseWishlist(wlUrl,browser)\n",
    "    \n",
    "    #Store Data\n",
    "    if items:\n",
    "        items['Url'] = [amazon + url for url in items['Url']]\n",
    "        itemTuple = zip([ID]*len(items['Price']),\n",
    "                       items['Name'],\n",
    "                       items['Price'],\n",
    "                       items['Url'])\n",
    "        cur.executemany(\"INSERT INTO items VALUES(?,?,?,?)\",itemTuple)\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute Function 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT Url, Name FROM items\")\n",
    "items = cur.fetchall()\n",
    "Categories = []\n",
    "\n",
    "for it in items:\n",
    "    url,Name = it\n",
    "    cat = getCategory(url,browser)\n",
    "    \n",
    "    \n",
    "    cur.execute('''\\\n",
    "    UPDATE items\\\n",
    "    SET Category=?\n",
    "    WHERE Name = ?;\n",
    "    ''',(cat,Name))\n",
    "    \n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon started captcha blocking me, but not before I got 14464 item categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x10650aa40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"Select Category from items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u', , , , , Clothing, Shoes & Jewelry, , , , \\u203a, , , , Women, , , , \\u203a, , , , Clothing, , , , \\u203a, , , , Tops & Tees, , , , \\u203a, , , , Knits & Tees, , , , , ',),\n",
       " (u', , , , , Clothing, Shoes & Jewelry, , , , \\u203a, , , , Women, , , , \\u203a, , , , Clothing, , , , \\u203a, , , , Tops & Tees, , , , \\u203a, , , , Blouses & Button-Down Shirts, , , , , ',),\n",
       " (u', , , , , Clothing, Shoes & Jewelry, , , , \\u203a, , , , Women, , , , \\u203a, , , , Clothing, , , , \\u203a, , , , Tops & Tees, , , , \\u203a, , , , Vests, , , , , ',),\n",
       " (u', , , , , Clothing, Shoes & Jewelry, , , , \\u203a, , , , Women, , , , \\u203a, , , , Clothing, , , , \\u203a, , , , Tops & Tees, , , , \\u203a, , , , Blouses & Button-Down Shirts, , , , , ',),\n",
       " (u', , , , , Clothing, Shoes & Jewelry, , , , \\u203a, , , , Women, , , , \\u203a, , , , Clothing, , , , \\u203a, , , , Tops & Tees, , , , \\u203a, , , , Blouses & Button-Down Shirts, , , , , ',)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = cur.fetchall()\n",
    "cat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u', , , , , Beauty & Personal Care, , , , \\u203a, , , , Hair Care, , , , \\u203a, , , , Styling Tools & Appliances, , , , \\u203a, , , , Hot-Air Brushes, , , , , ',)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat[14564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227640"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper was a success! Next step of this project will be finding a way to predict what gifts people want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "* [Why Certain Gifts Are Great to Give but Not to Get ](http://journals.sagepub.com/doi/pdf/10.1177/0963721416656937)\n",
    "* [Money canâ€™t buy love](https://doi.org/10.1016/j.jesp.2008.11.003)\n",
    "* [Give a piece of you](https://doi.org/10.1016/j.jesp.2015.04.006)\n",
    "* [Gift giving behavior](https://www.ideals.illinois.edu/bitstream/handle/2142/27449/giftgivingbehavi449belk.pdf?sequence=1)\n",
    "* [Sentimental value and gift giving](http://dx.doi.org/10.1016/j.jcps.2017.06.002 )\n",
    "* [How to choose a hobby](https://hobbylark.com/misc/How-to-Choose-a-Hobby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
